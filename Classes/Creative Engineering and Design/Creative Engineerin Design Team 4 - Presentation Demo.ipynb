{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"E3axBbwgBOVc","executionInfo":{"status":"ok","timestamp":1680192510361,"user_tz":-540,"elapsed":273,"user":{"displayName":"KOBILOV ILKHOMJON","userId":"07432962820947889943"}}},"outputs":[],"source":["import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from nltk.stem.porter import PorterStemmer\n","import torch, nltk, re, json, os, random\n","\n","\n","stemmer = PorterStemmer()\n","\n","def tokenize(sentence):\n","    return nltk.word_tokenize(sentence)\n","\n","def stem(word):\n","    return stemmer.stem(word.lower())\n","\n","def bag_of_words(tokenized_sentence, all_words):\n","    tokenized_sentence = [stem(w) for w in tokenized_sentence]\n","\n","    bag = np.zeros(len(all_words), dtype=np.float32)\n","    for idx, w in enumerate(all_words):\n","        if w in tokenized_sentence:\n","            bag[idx] = 1.0\n","\n","    return bag"]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Bz5bmAiCf6i","executionInfo":{"status":"ok","timestamp":1680192511217,"user_tz":-540,"elapsed":586,"user":{"displayName":"KOBILOV ILKHOMJON","userId":"07432962820947889943"}},"outputId":"536156f6-ee27-4c6a-a6c1-deb966520b5c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["class NeuralNet(nn.Module):\n","\tdef __init__(self, input_size, hidden_size, num_classes):\n","\t\tsuper(NeuralNet, self).__init__()\n","\t\tself.l1 = nn.Linear(input_size, hidden_size)\n","\t\tself.l2 = nn.Linear(hidden_size, hidden_size)\n","\t\tself.l3 = nn.Linear(hidden_size, num_classes)\n","\t\tself.relu = nn.ReLU()\n","\n","\tdef forward(self, x):\n","\t\tout = self.l1(x)\n","\t\tout = self.relu(out)\n","\t\tout = self.l2(out)\n","\t\tout = self.relu(out)\n","\t\tout = self.l3(out)\n","\n","\t\t#no activation no softmax\n","\n","\t\treturn out"],"metadata":{"id":"XWZLPkfOChjp","executionInfo":{"status":"ok","timestamp":1680192511217,"user_tz":-540,"elapsed":2,"user":{"displayName":"KOBILOV ILKHOMJON","userId":"07432962820947889943"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def train_model():\n","\twith open('intents.json') as f:\n","\t\tintents = json.load(f)\n","\n","\tall_words = []\n","\ttags = []\n","\txy = []\n","\n","\tfor intent in intents['intents']:\n","\t\ttag = intent['tag']\n","\t\ttags.append(tag)\n","\t\tfor pattern in intent['patterns']:\n","\t\t\tw = tokenize(pattern)\n","\t\t\tall_words.extend(w)\n","\t\t\txy.append((w, tag))\n","\n","\tignore_words = ['?', '!', ',', '.']\n","\tall_words = [stem(w) for w in all_words if w not in ignore_words]\n","\tall_words = sorted(set(all_words))\n","\ttags = sorted(set(tags))\n","\n","\tx_train = []\n","\ty_train = []\n","\n","\tfor (pattern_sentence, tag) in xy:\n","\t\tbag = bag_of_words(pattern_sentence, all_words)\n","\t\tx_train.append(bag)\n","\n","\t\tlabel = tags.index(tag)\n","\t\ty_train.append(label)\n","\n","\tx_train = np.array(x_train)\n","\ty_train = np.array(y_train)\n","\n","\n","\tclass ChatDataset(Dataset):\n","\t\tdef __init__(self):\n","\t\t\tself.n_samples = len(x_train)\n","\t\t\tself.x_data = x_train\n","\t\t\tself.y_data = y_train\n","\n","\t\tdef __getitem__(self, index):\n","\t\t\treturn self.x_data[index], self.y_data[index]\n","\n","\t\tdef __len__(self):\n","\t\t\treturn self.n_samples\n","\n","\n","\tbatch_size = 8\n","\thidden_size = 8\n","\toutput_size = len(tags)\n","\tinput_size = len(x_train[0])\n","\tlearning_rate = 0.001\n","\tnum_epochs = 1000\n","\n","\tdataset = ChatDataset()\n","\ttrain_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n","\n","\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\tmodel = NeuralNet(input_size, hidden_size, output_size).to(device)\n","\n","\t# Loss and optimizer\n","\tcriterion = nn.CrossEntropyLoss()\n","\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","\t# Train the model\n","\tfor epoch in range(num_epochs):\n","\t\tfor (words, labels) in train_loader:\n","\t\t\twords = words.to(device)\n","\t\t\tlabels = labels.to(dtype=torch.long).to(device)\n","\t\t\t\n","\t\t\toutputs = model(words)\n","\n","\t\t\tloss = criterion(outputs, labels)\n","\t\t\t\n","\t\t\toptimizer.zero_grad()\n","\t\t\tloss.backward()\n","\t\t\toptimizer.step()\n","\t\t\t\n","\t\tif (epoch+1) % 100 == 0:\n","\t\t\tprint (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","\n","\tprint(f'final loss: {loss.item():.4f}')\n","\n","\tdata = {\n","\t\"model_state\": model.state_dict(),\n","\t\"input_size\": input_size,\n","\t\"hidden_size\": hidden_size,\n","\t\"output_size\": output_size,\n","\t\"all_words\": all_words,\n","\t\"tags\": tags\n","\t}\n","\n","\t#os.remove(\"data.pth\")\n","\tFILE = f\"data.pth\"\n","\n","\ttorch.save(data, FILE)\n","\n","\tprint(f'Training is complete, here is the model {FILE}')\n"," \n","train_model()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GshXuGdYCk56","executionInfo":{"status":"ok","timestamp":1680192518696,"user_tz":-540,"elapsed":7481,"user":{"displayName":"KOBILOV ILKHOMJON","userId":"07432962820947889943"}},"outputId":"372ecc02-97f3-4e85-89b1-fc55afe7e828"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.3855\n","Epoch [200/1000], Loss: 0.0100\n","Epoch [300/1000], Loss: 0.0030\n","Epoch [400/1000], Loss: 0.0029\n","Epoch [500/1000], Loss: 0.0008\n","Epoch [600/1000], Loss: 0.0007\n","Epoch [700/1000], Loss: 0.0003\n","Epoch [800/1000], Loss: 0.0004\n","Epoch [900/1000], Loss: 0.0005\n","Epoch [1000/1000], Loss: 0.0002\n","final loss: 0.0002\n","Training is complete, here is the model data.pth\n"]}]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","with open('intents.json', 'r') as f:\n","    intents = json.load(f)\n","\n","FILE = \"data.pth\"\n","data = torch.load(FILE)\n","\n","input_size = data[\"input_size\"]\n","hidden_size = data[\"hidden_size\"]\n","output_size = data[\"output_size\"]\n","all_words = data[\"all_words\"]\n","tags = data[\"tags\"]\n","model_state = data[\"model_state\"]\n","\n","\n","model = NeuralNet(input_size, hidden_size, output_size).to(device)\n","model.load_state_dict(model_state)\n","model.eval()\n","\n","def get_response(msg):\n","\n","    sentence = tokenize(msg)\n","    X = bag_of_words(sentence, all_words)\n","    X = X.reshape(1, X.shape[0])\n","    X = torch.from_numpy(X).to(device)\n","\n","    output = model(X)\n","    _, predicted = torch.max(output, dim=1)\n","\n","    tag = tags[predicted.item()]\n","\n","    probs = torch.softmax(output, dim=1)\n","    prob = probs[0][predicted.item()]\n","\n","    if prob.item() > 0.75:\n","        for intent in intents['intents']:\n","            if tag == intent[\"tag\"]:\n","                answer = random.choice(intent['responses'])\n","                return answer\n","    else:\n","        return \"tushunmadim\"\n"],"metadata":{"id":"6gIH5KscColK","executionInfo":{"status":"ok","timestamp":1680192518696,"user_tz":-540,"elapsed":9,"user":{"displayName":"KOBILOV ILKHOMJON","userId":"07432962820947889943"}}},"execution_count":6,"outputs":[]}]}